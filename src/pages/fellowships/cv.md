---
layout: ../../layouts/SimpleLayout.astro
title: Computer Vision Fellowship
---

# AI Safety at UCLA Intro Fellowship: Computer Vision Track

## Table of Contents

1. [Week 1: Preventing an AI-related catastrophe](#week-1-preventing-an-ai-related-catastrophe)
2. [Week 2: The future is going to be wild](#week-2-the-future-is-going-to-be-wild)
3. [Week 3: AI Safety Field Background](#week-3-ai-safety-field-background)
4. [Week 4: Unsolved Problems in ML Safety](#week-4-unsolved-problems-in-ml-safety)
5. [Week 5: Failure Modes in AI](#week-5-failure-modes-in-ai)
6. [Week 6: Open Problems in AI X-Risk](#week-6-open-problems-in-ai-x-risk)

## Week 1: Preventing an AI-related catastrophe

Core readings: (120 mins)

1. [Preventing an AI-related catastrophe](https://80000hours.org/problem-profiles/artificial-intelligence/#top) (120 min)

Learning Goals:

1. Familiarize yourself with the arguments for AI being an existential risk
2. Understand why RL enables superhuman performance

## Week 2: The future is going to be wild

The progress of AI has been quite fast, AI today is quite capable, and AI has been very useful in solving problems that other methods cannot solve.

Core Content: 110 min

Conceptual Readings (60 mins):

1. [AI and Compute](https://openai.com/blog/ai-and-compute/) (5 min)
2. [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) (10 min)
3. [All Possible Views About Humanity’s Future are Wild](https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/) (15 min)
4. [This can’t go on](https://www.cold-takes.com/this-cant-go-on/) (20 min)
5. [Intelligence Explosion: Evidence and Import](https://drive.google.com/file/d/1QxMuScnYvyq-XmxYeqBRHKz7cZoOosHr/view) (20 min)

Technical Readings (50 min)

1. [But what is a Neural Network?](https://www.3blue1brown.com/lessons/neural-networks) (20 min)
2. [Gradient Descent, how neural networks learn](https://www.3blue1brown.com/lessons/gradient-descent) (20 min)
3. [Analyzing our neural network](https://www.3blue1brown.com/lessons/neural-network-analysis) (10 min)

Learning Goals:

1. Understand the relationship between compute and general capabilities.
2. Gain experience with the types of datasets used in modern AI systems.
3. See how AI could impact a wide range of industries.
4. Reflect on the radical impact AI can have on the future of humanity
5. Reflect on the strange possibilities of our economic future.
6. Reflect on the speed with which AI will transition from powerful to superintelligence.
7. Start to understand how neural networks work.

## Week 3: AI Safety Field Background

Core content: 105 min

Conceptual Readings (75 min):

1. [A Bird's Eye View of the ML Field](https://www.alignmentforum.org/s/FaEBwhhe3otzYKGQt/p/AtfQFj8umeyBBkkxa) (45 min)
2. [Paul Christiano: Current work in AI alignment](https://forum.effectivealtruism.org/posts/63stBTw3WAW6k45dY/paul-christiano-current-work-in-ai-alignment) (30 min)

Technical Readings (30 min):

1. [What is backpropagation really doing?](https://www.3blue1brown.com/lessons/backpropagation) (15 min)
2. [Backpropagation calculus](https://www.3blue1brown.com/lessons/backpropagation-calculus) (15 min)

Learning Goals:

1. Understand how ML research is conducted and how it affects AI safety research.
2. Be able to evaluate if a research agenda advances general capabilities.
3. Learn about the variety of different research approaches tackling alignment.
4. Understand how neural networks are trained in practice.
5. Get a grasp of the math behind “backprop.”

## Week 4: Unsolved Problems in ML Safety

Core content: 100 min

Conceptual readings (80 min):

1. [Unsolved Problems in ML Safety](https://arxiv.org/pdf/2109.13916.pdf) (60 min)
2. [Why AI alignment could be hard with modern deep learning](https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/) (20 mins)

Technical Content (20 min):

1. Read through the README in the [technical materials](https://github.com/ais-ucla/cv-fellowship), and follow the setup instructions, and start looking at the README in the micrograd folder too. Message your facilitator and/or bring your laptop to the meeting if you have issues!

Learning Goals:

1. Be able to determine how an AI safety project may reduce X-risk.
2. Evaluate the failure modes of misaligned AI.
3. Understand the factors that lead to value lock-in.
4. Get coding environment setup for ML projects.

## Week 5: Failure Modes in AI

Core content: < 100 min

Conceptual Readings (55 min):

1. [X-Risk Analysis for AI Research](https://arxiv.org/pdf/2206.05862) (Only read Appendix A) (10 min)
2. [What Failure Looks Like](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like) (10 min)
3. [Clarifying What Failure Looks Like](https://www.lesswrong.com/posts/v6Q7T335KCMxujhZu/clarifying-what-failure-looks-like-part-1) (25 mins)

Technical Content (max 45 min):

1. Complete Steps One and Two (optional: Step Three) of the [micrograd project](https://github.com/AIS-UCLA/cv-fellowship/tree/master/micrograd). Reach out to your facilitator with any questions, and use pytest, the solutions branch, and [Andrej Karpathy’s micrograd video](https://youtu.be/VMj-3S1tku0?si=hmx_3_ExUxo83QNY) for help as well! Don’t spend more than 45 minutes on this, especially if you feel stuck.

Learning Goals:

1. Understand issues with only using performance to evaluate classifiers.
2. Train a neural network completely from scratch!

## Week 6: Open Problems in AI X-Risk

Core content: < 105 min

Conceptual readings (60 min):

1. [Open Problems in AI X-Risk](https://www.alignmentforum.org/s/FaEBwhhe3otzYKGQt/p/5HtDzRAk7ePWsiL2L) (45 min)
2. [AI Governance: Opportunity and Theory of Impact](https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact) (15 min)

Technical content (max 45 min):

1. Complete steps One and Two of the [pytorch project](https://github.com/AIS-UCLA/cv-fellowship/tree/master/pytorch). Don’t spend more than 45 minutes on this, and ask for help if you feel stuck!
2. **Optionally:** Watch [But what is a convolution?](https://youtu.be/KuXjwB4LzSA) and complete Step Three of the pytorch project.

Learning Goals:

1. Pick a research agenda you find particularly interesting (perhaps to pursue later).
2. Understand the role AI governance plays in the broader field of AI safety.
3. Use PyTorch to create and train a neural network.
4. Optionally understand and implement a convolutional neural network in PyTorch.
